# daily_finance_news_app.pyï¼ˆå¤šä¾†æºæ–°èæ“´å……ç‰ˆï¼‰
import streamlit as st
import requests
from bs4 import BeautifulSoup

st.set_page_config(page_title="æ¯æ—¥è²¡ç¶“æ–°èå¿«å ±", layout="wide")
st.title("ğŸ“Š æ¯æ—¥è²¡ç¶“æ–°èå¿«å ±")

# ğŸ“° æ“·å– Reuters è²¡ç¶“æ–°è
@st.cache_data
def get_reuters_news():
    try:
        url = "https://www.reuters.com/finance/"
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        headlines = soup.find_all('h3')[:5]
        return [h.get_text().strip() for h in headlines if h.get_text().strip()]
    except:
        return ["ç„¡æ³•æ“·å– Reuters æ–°è"]

# ğŸ“° æ“·å– Bloomberg è²¡ç¶“æ–°èï¼ˆé¦–é æ¨™é¡Œï¼‰
@st.cache_data
def get_bloomberg_news():
    try:
        url = "https://www.bloomberg.com"
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        headlines = soup.find_all('h3')[:5]
        return [h.get_text().strip() for h in headlines if h.get_text().strip()]
    except:
        return ["ç„¡æ³•æ“·å– Bloomberg æ–°è"]

# ğŸ“° æ“·å– Yahoo Finance é ­æ¢æ–°è
@st.cache_data
def get_yahoo_news():
    try:
        url = "https://finance.yahoo.com"
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        headlines = soup.find_all('h3')[:5]
        return [h.get_text().strip() for h in headlines if h.get_text().strip()]
    except:
        return ["ç„¡æ³•æ“·å– Yahoo Finance æ–°è"]

# é¡¯ç¤ºå€å¡Š
st.header("ğŸ“° Reuters è²¡ç¶“æ–°è")
for i, news in enumerate(get_reuters_news(), 1):
    st.markdown(f"**{i}.** {news}")

st.header("ğŸ“° Bloomberg è²¡ç¶“æ–°è")
for i, news in enumerate(get_bloomberg_news(), 1):
    st.markdown(f"**{i}.** {news}")

st.header("ğŸ“° Yahoo Finance æ–°è")
for i, news in enumerate(get_yahoo_news(), 1):
    st.markdown(f"**{i}.** {news}")
